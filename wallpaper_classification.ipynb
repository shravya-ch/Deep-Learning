{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Term_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dxg5l5IgREQ9",
        "whzp3L51ROL9",
        "FduQhTyRy5Wi",
        "8l2X6D2DRl17"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "dxg5l5IgREQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import recall_score,precision_score,f1_score\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "1XICvmQt3GXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import timeit\n",
        "import unittest\n",
        "import random\n",
        "import torchvision.models as models\n",
        "## Please DONOT remove these lines. \n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "RqhjFpoyzS2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xg23vldT_aww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "whzp3L51ROL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!unzip ./drive/MyDrive/data-new.zip\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "neWIdkkfBo8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/data-new/wallpapers/train/.DS_Store\n",
        "!rm /content/data-new/wallpapers/test/.DS_Store\n",
        "!rm /content/data-new/wallpapers/.DS_Store\n",
        "#!mkdir /content/drive/MyDrive/data\n",
        "!find /content/data-new/ -name \\.DS_Store -delete"
      ],
      "metadata": {
        "id": "l4pUaOES2wt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir= '/content/drive/MyDrive/data';\n",
        "checkpointDir = 'modelCheckpoints';\n",
        "train_folder = 'train';\n",
        "test_folder  = 'test';\n",
        "Symmetry_Groups = ['P1', 'P2', 'PM' ,'PG', 'CM', 'PMM', 'PMG', 'PGG', 'CMM','P4', 'P4M', 'P4G', 'P3', 'P3M1', 'P31M', 'P6', 'P6M']"
      ],
      "metadata": {
        "id": "XbY44DHawBd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = dict(zip(Symmetry_Groups, range(17)))\n",
        "label_mapping"
      ],
      "metadata": {
        "id": "YU-eqkuG3TgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "for group in Symmetry_Groups:\n",
        "    files.extend(os.listdir(join(\"/content/data-new/wallpapers/train\",group)))\n",
        "s = pd.Series(files).apply(lambda x: x.split(\"_\")[0]).map(label_mapping)\n",
        "pd.concat([pd.Series(files),s],axis=1).to_csv(\"/content/drive/MyDrive/data/train_csv\")"
      ],
      "metadata": {
        "id": "wGxnDAnE2wpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "for group in Symmetry_Groups:\n",
        "    files.extend(os.listdir(join(\"/content/data-new/wallpapers/test\",group)))\n",
        "s = pd.Series(files).apply(lambda x: x.split(\"_\")[0]).map(label_mapping)\n",
        "pd.concat([pd.Series(files),s],axis=1).to_csv(\"/content/drive/MyDrive/data/test_csv\")"
      ],
      "metadata": {
        "id": "ibw7qq_aItT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check availability of GPU and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_cuda =  torch.cuda.init()# if you have acess to a GPU, enable it to speed the training "
      ],
      "metadata": {
        "id": "x_OvOpAszxrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class normalise(object):\n",
        "    def __call__(self,img):\n",
        "      img = torch.from_numpy(np.asarray(img))\n",
        "      img = img / 255\n",
        "      return img"
      ],
      "metadata": {
        "id": "h4CUFSJ5RjHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file,index_col=0)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0].split(\"_\")[0] +\"/\"+self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "-bczqALpT5n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomImageDataset(\"/content/drive/MyDrive/data/train_csv\",\"/content/data-new/wallpapers/train\" ,\n",
        "                                   transform=transforms.Compose([normalise()]))\n",
        "test_dataset  = CustomImageDataset(\"/content/drive/MyDrive/data/test_csv\",\"/content/data-new/wallpapers/test\",\n",
        "                                  transform=transforms.Compose([normalise()]))"
      ],
      "metadata": {
        "id": "0u7iVMTgYrNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders for training and test datasets\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=250,shuffle=True)\n",
        "#valid_dataloader = torch.utils.data.DataLoader(val_set,batch_size=250,shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=250,shuffle=True)\n"
      ],
      "metadata": {
        "id": "tam4p4o1or19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "im, target = next(iter(train_dataloader))\n",
        "print(im.shape, target.shape)\n",
        "plt.imshow(im[0].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0vOzXJ_mZrju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jitter = T.ColorJitter(brightness=.5, hue=.3)\n",
        "jitted_imgs = [jitter(im[0]) for _ in range(4)]\n",
        "fig, ax = plt.subplots(1, 4,figsize=(20,20))\n",
        "for i in range(4):\n",
        "  ax[i].imshow(jitted_imgs[i].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")"
      ],
      "metadata": {
        "id": "02RvkbAHTvZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotater = T.RandomRotation(degrees=(0, 360))\n",
        "rotated_imgs = [rotater(im[0]) for _ in range(4)]\n",
        "fig, ax = plt.subplots(1, 4,figsize=(20,20))\n",
        "for i in range(4):\n",
        "  ax[i].imshow(rotated_imgs[i].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")"
      ],
      "metadata": {
        "id": "_MoAREuMYwoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "affine_transfomer = T.RandomAffine(degrees=(0, 0), translate=(0.1, 0.3), scale=(1,2 ))\n",
        "affine_imgs = [affine_transfomer(im[0]) for _ in range(4)]\n",
        "fig, ax = plt.subplots(1, 4,figsize=(20,20))\n",
        "for i in range(4):\n",
        "  ax[i].imshow(affine_imgs[i].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")"
      ],
      "metadata": {
        "id": "EnvRDmZsZBnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blurrer = T.GaussianBlur(kernel_size=(3, 5), sigma=(0.1, 0.5))\n",
        "blurred_imgs = [blurrer(im[0]) for _ in range(4)]\n",
        "fig, ax = plt.subplots(1, 4,figsize=(20,20))\n",
        "for i in range(4):\n",
        "  ax[i].imshow(blurred_imgs[i].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")"
      ],
      "metadata": {
        "id": "JR_1X-NS8Ouo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "center_crops = [T.CenterCrop(size=size)(im[0]) for size in [(128,128)]]\n",
        "fig, ax = plt.subplots(1, len(center_crops))\n",
        "plt.imshow(center_crops[0].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")"
      ],
      "metadata": {
        "id": "V3pJo8kZUrZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = transforms.Compose([#T.ColorJitter(brightness=.5, hue=.3),\n",
        "                                   # T.GaussianBlur(kernel_size=(3, 5), sigma=(0.1, 0.5)),\n",
        "                                    T.RandomAffine(degrees=(0, 0), translate=(0.1, 0.3), scale=(1,2 )),\n",
        "                                    T.RandomRotation(degrees=(0, 360)),\n",
        "                                    T.CenterCrop(size=(256,256)),\n",
        "                                    normalise() ])"
      ],
      "metadata": {
        "id": "okkJOctxeHSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train_dataset = CustomImageDataset(\"/content/drive/MyDrive/data/train_csv\",\"/content/data-new/wallpapers/train\" ,\n",
        "                                   transform=augmentations)\n",
        "aug_test_dataset  = CustomImageDataset(\"/content/drive/MyDrive/data/test_csv\",\"/content/data-new/wallpapers/test\",\n",
        "                                  transform=augmentations)"
      ],
      "metadata": {
        "id": "_RSp4Yz_d327"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders for training and test datasets\n",
        "aug_train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=250,shuffle=True)\n",
        "#valid_dataloader = torch.utils.data.DataLoader(val_set,batch_size=250,shuffle=True)\n",
        "aug_test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=250,shuffle=True)\n"
      ],
      "metadata": {
        "id": "Lb8mV-NuiF3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "im, target = next(iter(aug_train_dataloader))\n",
        "print(im.shape, target.shape)\n",
        "fig, ax = plt.subplots(1, 4,figsize=(20,20))\n",
        "for i in range(4):\n",
        "  ax[i].imshow(im[i].squeeze(0).detach().cpu().numpy(),cmap=\"gray\")"
      ],
      "metadata": {
        "id": "lH5yv20BhO21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training functions"
      ],
      "metadata": {
        "id": "FduQhTyRy5Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subset(percent_of_data):\n",
        "  return torch.utils.data.Subset(aug_train_dataset, random.sample(range(0,len(aug_train_dataset)),\n",
        "                                                              int(percent_of_data*(len(aug_train_dataset)/100))) )"
      ],
      "metadata": {
        "id": "7YD1KoMxcGGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crs_entropy = nn.CrossEntropyLoss()\n",
        "def multi_metrics(y_pred, y_test):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
        "    \n",
        "    correct_pred = (y_pred_tags == y_test).float()\n",
        "    acc = correct_pred.sum() #/ len(correct_pred)\n",
        "    precision = precision_score(y_test.detach().cpu().numpy(),y_pred_tags.detach().cpu().numpy(),average=\"micro\")\n",
        "    recall = recall_score(y_test.detach().cpu().numpy(),y_pred_tags.detach().cpu().numpy(),average=\"micro\")\n",
        "    f1 = f1_score(y_test.detach().cpu().numpy(),y_pred_tags.detach().cpu().numpy(),average=\"micro\")\n",
        "    #acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc,precision,recall,f1,y_pred_tags\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch,):\n",
        "    model.train()\n",
        "   \n",
        "    loss_per_epoch = 0\n",
        "    correct = 0\n",
        "    total_prec = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    confusion_matrix = np.zeros((17, 17))\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      \n",
        "        inputs, labels = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = crs_entropy(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "  \n",
        "        train_acc,pre, recall, f1,y_pred = multi_metrics(output, labels)\n",
        "        correct += train_acc\n",
        "        total_prec += pre\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "\n",
        "        idx =  np.array((labels.view(-1).detach().cpu().numpy(),y_pred.view(-1).detach().cpu().numpy())).T\n",
        "        np.add.at(confusion_matrix,(idx[:,0],idx[:,1]),1)\n",
        "\n",
        "        if (batch_idx % 50 == 0) or (batch_idx == (len(train_loader)-1)):\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            print('\\nTrain Accuracy: {} [{}/{} ({:.0f}%)]\\tAccuracy:{}/{} ({:.0f}%)\\n'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
        "                100. * batch_idx / len(train_loader), correct, len(train_loader.sampler),\n",
        "        100. * correct / len(train_loader.sampler)))\n",
        "        loss_per_epoch += loss.item()\n",
        "\n",
        "    print('\\nTrain set: PRECISION: {:.4f}'.format(100. * total_prec / len(train_loader)))\n",
        "    print('\\nTrain set: RECALL: {:.4f}'.format(100. * total_recall / len(train_loader)))\n",
        "    print('\\nTrain set: F1: {:.4f}'.format(100. * total_f1 / len(train_loader)))\n",
        "\n",
        "    return loss_per_epoch,100* correct/len(train_loader.sampler),confusion_matrix\n",
        "    "
      ],
      "metadata": {
        "id": "PDWW-rrWa2uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_prec = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    confusion_matrix = np.zeros((17, 17))\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            image,target = data.to(device), target.to(device)\n",
        "            output = model(image)\n",
        "            test_loss += crs_entropy(output, target).item()  \n",
        "            test_acc,pre, recall, f1 ,y_pred= multi_metrics(output, target)\n",
        "            correct += test_acc\n",
        "            total_prec += pre\n",
        "            total_recall += recall\n",
        "            total_f1 += f1\n",
        "\n",
        "            idx =  np.array((target.view(-1).detach().cpu().numpy(),y_pred.view(-1).detach().cpu().numpy())).T\n",
        "            np.add.at(confusion_matrix,(idx[:,0],idx[:,1]),1)\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.sampler),\n",
        "        100. * correct / len(test_loader.sampler)))\n",
        "    \n",
        "    print('\\nTest set: PRECISION: {:.4f}'.format(100. * total_prec / len(test_loader)))\n",
        "    print('\\nTest set: RECALL: {:.4f}'.format(100. * total_recall / len(test_loader)))\n",
        "    print('\\nTest set: F1: {:.4f}'.format(100. * total_f1 / len(test_loader)))\n",
        "    \n",
        "    return test_loss,100. * correct / len(test_loader.sampler), confusion_matrix\n",
        "   "
      ],
      "metadata": {
        "id": "EOoHlwDOXDPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "1MjDMdCdRYP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "k4AE5dWMRfSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_model_resent():\n",
        "  resnet_model = models.resnet18(pretrained=False)\n",
        "  resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "  for param in resnet_model.parameters():\n",
        "    param.requires_grad = True\n",
        "  num_ftrs = resnet_model.fc.in_features\n",
        "  resnet_model.fc = nn.Linear(num_ftrs, len(Symmetry_Groups))\n",
        "  resnet_model = resnet_model.to(device)\n",
        "  params_to_update = []\n",
        "  for name,param in resnet_model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "      # print(\"\\t\",name)\n",
        "  return resnet_model,params_to_update"
      ],
      "metadata": {
        "id": "0Dq1AuiBQODF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "batch_size = 128\n",
        "\n",
        "avg_train_acc= []\n",
        "avg_valid_acc = []   \n",
        "avg_train_loss = []\n",
        "avg_valid_loss = [] \n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "train_subset = get_subset(20)\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_subset)):\n",
        "    print(f'FOLD {fold}')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      train_subset, \n",
        "                      batch_size=batch_size, sampler=train_subsampler)\n",
        "    \n",
        "    validloader = torch.utils.data.DataLoader(\n",
        "                      train_subset,\n",
        "                      batch_size=batch_size, sampler=valid_subsampler)\n",
        "    \n",
        "    train_loss_per_epoch = []\n",
        "\n",
        "    model,params_to_update = reset_model_resent()\n",
        "    model = model.to(device)\n",
        "    optimizer =  optim.Adam(params_to_update, lr=0.0001)\n",
        "    \n",
        "    start = timeit.default_timer()\n",
        "    for epoch in range(1, 2):  \n",
        "        train_loss,train_acc,cf_train =  train(model, device, trainloader, optimizer, epoch)\n",
        "        train_loss_per_epoch.append(train_loss)\n",
        "\n",
        "    #train_loss,train_acc,cf_train = (test(model, device, trainloader, criterion))    \n",
        "    valid_loss,valid_acc,cf_valid = (test(model, device, validloader, criterion))\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    print('Total time taken: {} seconds'.format(int(stop - start)) )\n",
        "\n",
        "    avg_train_acc.append(train_acc.item())\n",
        "    avg_valid_acc.append(valid_acc.item())  \n",
        "    avg_train_loss.append(train_loss)\n",
        "    avg_valid_loss.append(valid_loss)\n",
        "\n",
        "    # Saving the model\n",
        "    model_name = \"Resnet\"\n",
        "    save_path = \"/content/drive/MyDrive/data\"+f'/model-{model_name}-fold-{fold}.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    for heatmap in [cf_train,cf_valid]:\n",
        "      plt.figure(figsize=(15,10))\n",
        "      sns.heatmap(heatmap,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "      plt.ylabel('True class')\n",
        "      plt.xlabel('Predicted class')\n",
        "      plt.show()\n",
        "\n",
        "print(f'Train All Loss: {np.mean(avg_train_loss):.5f} | Train All Acc: {np.mean(avg_train_acc):.3f}| STD: {np.std(avg_train_loss)} | STD: {np.std(avg_train_acc)}')\n",
        "print(f'Validation All Loss: {np.mean(avg_valid_loss):.5f} | Validation All Acc: {np.mean(avg_valid_acc):.3f}| STD: {np.std(avg_valid_loss)} | STD: {np.std(avg_valid_acc)} ')\n"
      ],
      "metadata": {
        "id": "YzMqSvw4P_-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model,_ = reset_model_resent()\n",
        "test_model.load_state_dict(torch.load(\"/content/drive/MyDrive/data/model-Resnet-fold-\"+str(np.argmax(avg_valid_acc))+\".pth\"))\n",
        "test_model.to(device)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=True)\n",
        "test_loss,test_acc,cf_test= (test(test_model, device, testloader, criterion=nn.CrossEntropyLoss()))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(cf_test,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_7T2Fh0oxQkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs = []\n",
        "test_targets=[]\n",
        "with torch.no_grad():\n",
        "      for data, target in testloader:\n",
        "          image,target = data.to(device), target.to(device)\n",
        "          output = test_model(image)\n",
        "          test_outputs.extend( output.detach().cpu().tolist())\n",
        "          test_targets.extend(target.detach().cpu().tolist())\n",
        "          \n",
        "tsne = TSNE(2, verbose=1)\n",
        "reduced_dim = tsne.fit_transform(test_outputs)\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "scatter = ax.scatter(reduced_dim[:,0],reduced_dim[:,1],c = test_targets)\n",
        "legend1 = ax.legend(*scatter.legend_elements(num=17),\n",
        "                    loc=\"lower left\", title=\"Classes\")\n",
        "ax.add_artist(legend1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nIuouEsBUFjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "test_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = train_dataset[4]\n",
        "data=data.to(device)\n",
        "data.unsqueeze_(0)\n",
        "output = test_model(data)\n",
        "\n",
        "k=0\n",
        "act = activation['conv1'].squeeze()\n",
        "fig,ax = plt.subplots(8,8,figsize=(12, 15))\n",
        "\n",
        "for i in range(act.size(0)//8):\n",
        "        for j in range(act.size(0)//8):\n",
        "           ax[i,j].imshow(act[k].detach().cpu().numpy())\n",
        "           k+=1    \n"
      ],
      "metadata": {
        "id": "uaSHu0_Nv6RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG16"
      ],
      "metadata": {
        "id": "uzTnMnWvVLae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg16_model():\n",
        "  vgg_model = models.vgg16(pretrained=False)\n",
        "  vgg_model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  for param in vgg_model.parameters():\n",
        "    param.requires_grad = True\n",
        "  num_ftrs = vgg_model.classifier[6].in_features\n",
        "  vgg_model.classifier[6] = nn.Linear(num_ftrs, len(Symmetry_Groups))\n",
        "  vgg_model = vgg_model.to(device)\n",
        "  params_to_update = []\n",
        "  for name,param in vgg_model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "      #print(\"\\t\",name)\n",
        "  return vgg_model,params_to_update"
      ],
      "metadata": {
        "id": "om5FH4P_VQtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "batch_size = 256\n",
        "\n",
        "avg_train_acc= []\n",
        "avg_valid_acc = []   \n",
        "avg_train_loss = []\n",
        "avg_valid_loss = [] \n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "train_subset = get_subset(20)\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_subset)):\n",
        "    print(f'FOLD {fold}')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      train_subset, \n",
        "                      batch_size=batch_size, sampler=train_subsampler)\n",
        "    \n",
        "    validloader = torch.utils.data.DataLoader(\n",
        "                      train_subset,\n",
        "                      batch_size=batch_size, sampler=valid_subsampler)\n",
        "    \n",
        "    train_loss_per_epoch = []\n",
        "\n",
        "    vgg_model,params_to_update = vgg16_model()\n",
        "    vgg_model = vgg_model.to(device)\n",
        "    optimizer =  optim.Adam(params_to_update, lr=0.0001)\n",
        "    \n",
        "    start = timeit.default_timer()\n",
        "    for epoch in range(1, 2):  \n",
        "        train_loss,train_acc,cf_train =  train(vgg_model, device, trainloader, optimizer, epoch)\n",
        "        train_loss_per_epoch.append(train_loss)\n",
        "\n",
        "    #train_loss,train_acc,cf_train = (test(model, device, trainloader, criterion))    \n",
        "    valid_loss,valid_acc,cf_valid = (test(vgg_model, device, validloader, criterion))\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    print('Total time taken: {} seconds'.format(int(stop - start)) )\n",
        "\n",
        "    avg_train_acc.append(train_acc.item())\n",
        "    avg_valid_acc.append(valid_acc.item())  \n",
        "    avg_train_loss.append(train_loss)\n",
        "    avg_valid_loss.append(valid_loss)\n",
        "\n",
        "    # Saving the model\n",
        "    model_name = \"Vgg16\"\n",
        "    save_path = \"/content/drive/MyDrive/data\"+f'/model-{model_name}-fold-{fold}.pth'\n",
        "    torch.save(vgg_model.state_dict(), save_path)\n",
        "    for heatmap in [cf_train,cf_valid]:\n",
        "      plt.figure(figsize=(15,10))\n",
        "      sns.heatmap(heatmap,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "      plt.ylabel('True class')\n",
        "      plt.xlabel('Predicted class')\n",
        "      plt.show()\n",
        "\n",
        "print(f'Train All Loss: {np.mean(avg_train_loss):.5f} | Train All Acc: {np.mean(avg_train_acc):.3f}| STD: {np.std(avg_train_loss)} | STD: {np.std(avg_train_acc)}')\n",
        "print(f'Validation All Loss: {np.mean(avg_valid_loss):.5f} | Validation All Acc: {np.mean(avg_valid_acc):.3f}| STD: {np.std(avg_valid_loss)} | STD: {np.std(avg_valid_acc)} ')\n"
      ],
      "metadata": {
        "id": "2J4-ri00Qteb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model,_ = vgg16_model()\n",
        "test_model.load_state_dict(torch.load(\"/content/drive/MyDrive/data/model-Vgg16-fold-\"+str(np.argmax(avg_valid_acc))+\".pth\"))\n",
        "test_model.to(device)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=True)\n",
        "test_loss,test_acc,cf_test= (test(test_model, device, testloader, criterion=nn.CrossEntropyLoss()))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(cf_test,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R6g7SkH4Qtee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs = []\n",
        "test_targets=[]\n",
        "with torch.no_grad():\n",
        "      for data, target in testloader:\n",
        "          image,target = data.to(device), target.to(device)\n",
        "          output = test_model(image)\n",
        "          test_outputs.extend( output.detach().cpu().tolist())\n",
        "          test_targets.extend(target.detach().cpu().tolist())\n",
        "          \n",
        "tsne = TSNE(2, verbose=1)\n",
        "reduced_dim = tsne.fit_transform(test_outputs)\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "scatter = ax.scatter(reduced_dim[:,0],reduced_dim[:,1],c = test_targets)\n",
        "legend1 = ax.legend(*scatter.legend_elements(num=17),\n",
        "                    loc=\"lower left\", title=\"Classes\")\n",
        "ax.add_artist(legend1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LlE6NUphQtee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "test_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = train_dataset[4]\n",
        "data=data.to(device)\n",
        "data.unsqueeze_(0)\n",
        "output = test_model(data)\n",
        "\n",
        "k=0\n",
        "act = activation['conv1'].squeeze()\n",
        "fig,ax = plt.subplots(8,8,figsize=(12, 15))\n",
        "\n",
        "for i in range(act.size(0)//8):\n",
        "        for j in range(act.size(0)//8):\n",
        "           ax[i,j].imshow(act[k].detach().cpu().numpy())\n",
        "           k+=1    \n"
      ],
      "metadata": {
        "id": "R_zcZFBAQtef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlexNet"
      ],
      "metadata": {
        "id": "8VY6h6ToVRio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alexnet_create_model():\n",
        "  alexnet_model = models.alexnet(pretrained=False)\n",
        "  alexnet_model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  for param in alexnet_model.parameters():\n",
        "    param.requires_grad = True\n",
        "  num_ftrs = alexnet_model.classifier[6].in_features\n",
        "  alexnet_model.classifier[6] = nn.Linear(num_ftrs, len(Symmetry_Groups))\n",
        "  alexnet_model = alexnet_model.to(device)\n",
        "  params_to_update = []\n",
        "  for name,param in alexnet_model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "      #print(\"\\t\",name)\n",
        "  return alexnet_model,params_to_update"
      ],
      "metadata": {
        "id": "SEaoSQAUVVdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "batch_size = 256\n",
        "\n",
        "avg_train_acc= []\n",
        "avg_valid_acc = []   \n",
        "avg_train_loss = []\n",
        "avg_valid_loss = [] \n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "train_subset = get_subset(20)\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_subset)):\n",
        "    print(f'FOLD {fold}')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      train_subset, \n",
        "                      batch_size=batch_size, sampler=train_subsampler)\n",
        "    \n",
        "    validloader = torch.utils.data.DataLoader(\n",
        "                      train_subset,\n",
        "                      batch_size=batch_size, sampler=valid_subsampler)\n",
        "    \n",
        "    train_loss_per_epoch = []\n",
        "\n",
        "    alexnet_model,params_to_update = alexnet_create_model()\n",
        "    alexnet_model = alexnet_model.to(device)\n",
        "    optimizer =  optim.Adam(params_to_update, lr=0.0001)\n",
        "    \n",
        "    start = timeit.default_timer()\n",
        "    for epoch in range(1, 2):  \n",
        "        train_loss,train_acc,cf_train =  train(alexnet_model, device, trainloader, optimizer, epoch)\n",
        "        train_loss_per_epoch.append(train_loss)\n",
        "\n",
        "    #train_loss,train_acc,cf_train = (test(model, device, trainloader, criterion))    \n",
        "    valid_loss,valid_acc,cf_valid = (test(alexnet_model, device, validloader, criterion))\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    print('Total time taken: {} seconds'.format(int(stop - start)) )\n",
        "\n",
        "    avg_train_acc.append(train_acc.item())\n",
        "    avg_valid_acc.append(valid_acc.item())  \n",
        "    avg_train_loss.append(train_loss)\n",
        "    avg_valid_loss.append(valid_loss)\n",
        "\n",
        "    # Saving the model\n",
        "    model_name = \"alexnet\"\n",
        "    save_path = \"/content/drive/MyDrive/data\"+f'/model-{model_name}-fold-{fold}.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    for heatmap in [cf_train,cf_valid]:\n",
        "      plt.figure(figsize=(15,10))\n",
        "      sns.heatmap(heatmap,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "      plt.ylabel('True class')\n",
        "      plt.xlabel('Predicted class')\n",
        "      plt.show()\n",
        "\n",
        "print(f'Train All Loss: {np.mean(avg_train_loss):.5f} | Train All Acc: {np.mean(avg_train_acc):.3f}| STD: {np.std(avg_train_loss)} | STD: {np.std(avg_train_acc)}')\n",
        "print(f'Validation All Loss: {np.mean(avg_valid_loss):.5f} | Validation All Acc: {np.mean(avg_valid_acc):.3f}| STD: {np.std(avg_valid_loss)} | STD: {np.std(avg_valid_acc)} ')\n"
      ],
      "metadata": {
        "id": "6KHCmqaKRjDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(avg_valid_acc))"
      ],
      "metadata": {
        "id": "uvNUv9v0RjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model,_ = alexnet_create_model()\n",
        "test_model.load_state_dict(torch.load(\"/content/drive/MyDrive/data/model-alexnet-fold-\"+str(np.argmax(avg_valid_acc))+\".pth\"))\n",
        "test_model.to(device)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=True)\n",
        "test_loss,test_acc,cf_test= (test(test_model, device, testloader, criterion=nn.CrossEntropyLoss()))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(cf_test,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5kcgU-SMRjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs = []\n",
        "test_targets=[]\n",
        "with torch.no_grad():\n",
        "      for data, target in testloader:\n",
        "          image,target = data.to(device), target.to(device)\n",
        "          output = test_model(image)\n",
        "          test_outputs.extend( output.detach().cpu().tolist())\n",
        "          test_targets.extend(target.detach().cpu().tolist())\n",
        "          \n",
        "tsne = TSNE(2, verbose=1)\n",
        "reduced_dim = tsne.fit_transform(test_outputs)\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "scatter = ax.scatter(reduced_dim[:,0],reduced_dim[:,1],c = test_targets)\n",
        "legend1 = ax.legend(*scatter.legend_elements(num=17),\n",
        "                    loc=\"lower left\", title=\"Classes\")\n",
        "ax.add_artist(legend1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MdKJy913RjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "test_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = train_dataset[4]\n",
        "data=data.to(device)\n",
        "data.unsqueeze_(0)\n",
        "output = test_model(data)\n",
        "\n",
        "k=0\n",
        "act = activation['conv1'].squeeze()\n",
        "fig,ax = plt.subplots(8,8,figsize=(12, 15))\n",
        "\n",
        "for i in range(act.size(0)//8):\n",
        "        for j in range(act.size(0)//8):\n",
        "           ax[i,j].imshow(act[k].detach().cpu().numpy())\n",
        "           k+=1    \n"
      ],
      "metadata": {
        "id": "gL1FPSVHRjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l2X6D2DRl17"
      },
      "source": [
        "# Custom/ Designed Networks "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wide Network"
      ],
      "metadata": {
        "id": "ti8Kp8GL0ex7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZXPVF5_avNS"
      },
      "outputs": [],
      "source": [
        "class BasicBlock_wide(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self):\n",
        "        super(BasicBlock_wide, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32,\n",
        "                                            kernel_size = 7, padding = (2,2), stride = (1,1), bias = True) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64,\n",
        "                                            kernel_size = 5, padding = (1,1), stride = (1,1), bias = True) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128,\n",
        "                                            kernel_size = 3, padding = (1,1), stride = (1,1), bias = True) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(28800, 512)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.linear2 = nn.Linear(512, 17)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wide_nw_create_model():\n",
        "  wide_nw  = BasicBlock_wide()\n",
        "  for param in wide_nw.parameters():\n",
        "    param.requires_grad = True\n",
        "  \n",
        "  wide_nw = wide_nw.to(device)\n",
        "  params_to_update = []\n",
        "  for name,param in wide_nw.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "      #print(\"\\t\",name)\n",
        "  return wide_nw,params_to_update"
      ],
      "metadata": {
        "id": "UcmhRllU1ZA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThChv5OCzHkZ"
      },
      "outputs": [],
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "batch_size = 256\n",
        "\n",
        "avg_train_acc= []\n",
        "avg_valid_acc = []   \n",
        "avg_train_loss = []\n",
        "avg_valid_loss = [] \n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "train_subset = get_subset(40)\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_subset)):\n",
        "    print(f'FOLD {fold}')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      train_subset, \n",
        "                      batch_size=batch_size, sampler=train_subsampler)\n",
        "    \n",
        "    validloader = torch.utils.data.DataLoader(\n",
        "                      train_subset,\n",
        "                      batch_size=batch_size, sampler=valid_subsampler)\n",
        "    \n",
        "    train_loss_per_epoch = []\n",
        "\n",
        "    wide_model,params_to_update = wide_nw_create_model()\n",
        "    wide_model = wide_model.to(device)\n",
        "    optimizer =  optim.Adam(params_to_update, lr=0.0001)\n",
        "    \n",
        "    start = timeit.default_timer()\n",
        "    for epoch in range(1, 2):  \n",
        "        train_loss,train_acc,cf_train =  train(wide_model, device, trainloader, optimizer, epoch)\n",
        "        train_loss_per_epoch.append(train_loss)\n",
        "\n",
        "    #train_loss,train_acc,cf_train = (test(model, device, trainloader, criterion))    \n",
        "    valid_loss,valid_acc,cf_valid = (test(wide_model, device, validloader, criterion))\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    print('Total time taken: {} seconds'.format(int(stop - start)) )\n",
        "\n",
        "    avg_train_acc.append(train_acc.item())\n",
        "    avg_valid_acc.append(valid_acc.item())  \n",
        "    avg_train_loss.append(train_loss)\n",
        "    avg_valid_loss.append(valid_loss)\n",
        "\n",
        "    # Saving the model\n",
        "    model_name = \"wide_nw\"\n",
        "    save_path = \"/content/drive/MyDrive/data\"+f'/model-{model_name}-fold-{fold}-40.pth'\n",
        "    torch.save(wide_model.state_dict(), save_path)\n",
        "    for heatmap in [cf_train,cf_valid]:\n",
        "      plt.figure(figsize=(15,10))\n",
        "      sns.heatmap(heatmap,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "      plt.ylabel('True class')\n",
        "      plt.xlabel('Predicted class')\n",
        "      plt.show()\n",
        "\n",
        "print(f'Train All Loss: {np.mean(avg_train_loss):.5f} | Train All Acc: {np.mean(avg_train_acc):.3f}| STD: {np.std(avg_train_loss)} | STD: {np.std(avg_train_acc)}')\n",
        "print(f'Validation All Loss: {np.mean(avg_valid_loss):.5f} | Validation All Acc: {np.mean(avg_valid_acc):.3f}| STD: {np.std(avg_valid_loss)} | STD: {np.std(avg_valid_acc)} ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYQYRFQmzHkb"
      },
      "outputs": [],
      "source": [
        "test_model,_ = wide_nw_create_model()\n",
        "test_model.load_state_dict(torch.load(\"/content/drive/MyDrive/data/model-wide_nw-fold-\"+str(np.argmax(avg_valid_acc))+\"-40.pth\"))\n",
        "test_model.to(device)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=True)\n",
        "test_loss,test_acc,cf_test= (test(test_model, device, testloader, criterion=nn.CrossEntropyLoss()))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(cf_test,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S5lkD3fzHkb"
      },
      "outputs": [],
      "source": [
        "test_outputs = []\n",
        "test_targets=[]\n",
        "with torch.no_grad():\n",
        "      for data, target in testloader:\n",
        "          image,target = data.to(device), target.to(device)\n",
        "          output = test_model(image)\n",
        "          test_outputs.extend( output.detach().cpu().tolist())\n",
        "          test_targets.extend(target.detach().cpu().tolist())\n",
        "          \n",
        "tsne = TSNE(2, verbose=1)\n",
        "reduced_dim = tsne.fit_transform(test_outputs)\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "scatter = ax.scatter(reduced_dim[:,0],reduced_dim[:,1],c = test_targets)\n",
        "legend1 = ax.legend(*scatter.legend_elements(num=17),\n",
        "                    loc=\"lower left\", title=\"Classes\")\n",
        "ax.add_artist(legend1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDiI4I5kzHkb"
      },
      "outputs": [],
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "test_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = train_dataset[4]\n",
        "data=data.to(device)\n",
        "data.unsqueeze_(0)\n",
        "output = test_model(data)\n",
        "\n",
        "k=0\n",
        "act = activation['conv1'].squeeze()\n",
        "fig,ax = plt.subplots(6,6,figsize=(12, 15))\n",
        "\n",
        "# for i in range(act.size(0)//6):\n",
        "#         for j in range(act.size(0)//6):\n",
        "#            ax[i,j].imshow(act[k].detach().cpu().numpy())\n",
        "#            k+=1    \n",
        "\n",
        "for act, ax in zip(act, ax.ravel()):\n",
        "    ax.imshow(act.detach().cpu().numpy())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "act.size()"
      ],
      "metadata": {
        "id": "XqwNwPh_aeD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skinny Network"
      ],
      "metadata": {
        "id": "sNVWhgRa4tiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR4Ajmkg4tiu"
      },
      "outputs": [],
      "source": [
        "class BasicBlock_skinny(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self):\n",
        "        super(BasicBlock_skinny, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 36,\n",
        "                                            kernel_size = 5, padding = (2,2), stride = (1,1), bias = True) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels = 36, out_channels = 36,\n",
        "                                            kernel_size = 5, padding = (1,1), stride = (1,1), bias = True) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels = 36, out_channels = 36,\n",
        "                                            kernel_size = 3, padding = (1,1), stride = (1,1), bias = True) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(8100, 128)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.linear2 = nn.Linear(128, 17) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def skinny_nw_create_model():\n",
        "  skinny_nw  = BasicBlock_skinny()\n",
        "  for param in skinny_nw.parameters():\n",
        "    param.requires_grad = True\n",
        "  \n",
        "  skinny_nw = skinny_nw.to(device)\n",
        "  params_to_update = []\n",
        "  for name,param in skinny_nw.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "      #print(\"\\t\",name)\n",
        "  return skinny_nw,params_to_update"
      ],
      "metadata": {
        "id": "B8AUnTol4tiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dp8s0YY4tiu"
      },
      "outputs": [],
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "batch_size = 256\n",
        "\n",
        "avg_train_acc= []\n",
        "avg_valid_acc = []   \n",
        "avg_train_loss = []\n",
        "avg_valid_loss = [] \n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "train_subset = get_subset(40)\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_subset)):\n",
        "    print(f'FOLD {fold}')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      train_subset, \n",
        "                      batch_size=batch_size, sampler=train_subsampler)\n",
        "    \n",
        "    validloader = torch.utils.data.DataLoader(\n",
        "                      train_subset,\n",
        "                      batch_size=batch_size, sampler=valid_subsampler)\n",
        "    \n",
        "    train_loss_per_epoch = []\n",
        "\n",
        "    skinny_model,params_to_update = skinny_nw_create_model()\n",
        "    skinny_model = skinny_model.to(device)\n",
        "    optimizer =  optim.Adam(params_to_update, lr=0.0001)\n",
        "    \n",
        "    start = timeit.default_timer()\n",
        "    for epoch in range(1, 2):  \n",
        "        train_loss,train_acc,cf_train =  train(skinny_model, device, trainloader, optimizer, epoch)\n",
        "        train_loss_per_epoch.append(train_loss)\n",
        "\n",
        "    #train_loss,train_acc,cf_train = (test(model, device, trainloader, criterion))    \n",
        "    valid_loss,valid_acc,cf_valid = (test(skinny_model, device, validloader, criterion))\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    print('Total time taken: {} seconds'.format(int(stop - start)) )\n",
        "\n",
        "    avg_train_acc.append(train_acc.item())\n",
        "    avg_valid_acc.append(valid_acc.item())  \n",
        "    avg_train_loss.append(train_loss)\n",
        "    avg_valid_loss.append(valid_loss)\n",
        "\n",
        "    # Saving the model\n",
        "    model_name = \"skinny_nw\"\n",
        "    save_path = \"/content/drive/MyDrive/data\"+f'/model-{model_name}-fold-{fold}-40.pth'\n",
        "    torch.save(skinny_model.state_dict(), save_path)\n",
        "    for heatmap in [cf_train,cf_valid]:\n",
        "      plt.figure(figsize=(15,10))\n",
        "      sns.heatmap(heatmap,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "      plt.ylabel('True class')\n",
        "      plt.xlabel('Predicted class')\n",
        "      plt.show()\n",
        "\n",
        "print(f'Train All Loss: {np.mean(avg_train_loss):.5f} | Train All Acc: {np.mean(avg_train_acc):.3f}| STD: {np.std(avg_train_loss)} | STD: {np.std(avg_train_acc)}')\n",
        "print(f'Validation All Loss: {np.mean(avg_valid_loss):.5f} | Validation All Acc: {np.mean(avg_valid_acc):.3f}| STD: {np.std(avg_valid_loss)} | STD: {np.std(avg_valid_acc)} ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIQOOfg74tiv"
      },
      "outputs": [],
      "source": [
        "test_model,_ = skinny_nw_create_model()\n",
        "test_model.load_state_dict(torch.load(\"/content/drive/MyDrive/data/model-skinny_nw-fold-\"+str(np.argmax(avg_valid_acc))+\"-40.pth\"))\n",
        "test_model.to(device)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=True)\n",
        "test_loss,test_acc,cf_test= (test(test_model, device, testloader, criterion=nn.CrossEntropyLoss()))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(cf_test,annot=True,fmt=\"g\",xticklabels=Symmetry_Groups,yticklabels=Symmetry_Groups)\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIJ4Kcz44tiv"
      },
      "outputs": [],
      "source": [
        "test_outputs = []\n",
        "test_targets=[]\n",
        "with torch.no_grad():\n",
        "      for data, target in testloader:\n",
        "          image,target = data.to(device), target.to(device)\n",
        "          output = test_model(image)\n",
        "          test_outputs.extend( output.detach().cpu().tolist())\n",
        "          test_targets.extend(target.detach().cpu().tolist())\n",
        "          \n",
        "tsne = TSNE(2, verbose=1)\n",
        "reduced_dim = tsne.fit_transform(test_outputs)\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "scatter = ax.scatter(reduced_dim[:,0],reduced_dim[:,1],c = test_targets)\n",
        "legend1 = ax.legend(*scatter.legend_elements(num=17),\n",
        "                    loc=\"lower left\", title=\"Classes\")\n",
        "ax.add_artist(legend1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d6BonlI4tiv"
      },
      "outputs": [],
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "test_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "data, _ = train_dataset[4]\n",
        "data=data.to(device)\n",
        "data.unsqueeze_(0)\n",
        "output = test_model(data)\n",
        "\n",
        "k=0\n",
        "act = activation['conv1'].squeeze()\n",
        "fig,ax = plt.subplots(6,6,figsize=(12, 15))\n",
        "\n",
        "# for i in range(act.size(0)//6):\n",
        "#         for j in range(act.size(0)//6):\n",
        "#            ax[i,j].imshow(act[k].detach().cpu().numpy())\n",
        "#            k+=1    \n",
        "\n",
        "for act, ax in zip(act, ax.ravel()):\n",
        "    ax.imshow(act.detach().cpu().numpy())\n",
        "\n",
        "plt.show()"
      ]
    }
  ]
}