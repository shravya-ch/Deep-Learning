# -*- coding: utf-8 -*-
"""NLP_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O5F396lB0cVkPpyFV46Ut9XkPdfcS787

# Install and Imports
"""

!pip install emoji

!pip install transformers

!pip install pyiwn
import pyiwn

pip install -U pytorch-pretrained-bert

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
import re
import emoji
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import transformers
from transformers import AutoModel, BertTokenizerFast,XLMRobertaTokenizerFast
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

"""# EDA"""

training_data = pd.read_csv("/content/constraint_Hindi_Train - Sheet1.csv")

training_data.head()

training_data.shape

training_data.columns

training_data.isnull().sum()

training_data.info()

training_data['Labels Set'].value_counts()

set().union(*(training_data['Labels Set'].apply(lambda x: x.split(","))))

classes = dict.fromkeys({'defamation', 'fake', 'hate', 'non-hostile', 'offensive'},0)

##count plot of each class for hostile and non-hostile
def counts(x):
  for each in x.split(","):
    classes[each] += 1
training_data['Labels Set'].apply(lambda x: counts(x))
sns.barplot(x = list(classes.values()),y = list(classes.keys() ))
plt.show()

plt.pie(training_data['Labels Set'].apply(lambda x: 0 if x == "non-hostile" else 1).value_counts(),
        labels= ["non-hostile","hostile"])
plt.show()

training_data["coarse_target"] = training_data['Labels Set'].apply(lambda x: 0 if x == "non-hostile" else 1)

for each in ['defamation', 'fake', 'hate', 'offensive']:
  training_data[each]= training_data['Labels Set'].str.contains(each).apply(lambda x : 1 if x== True else 0)

training_data.head()

tokens = training_data["Post"].apply(lambda x : x.split(" "))
tokens

def subs(text):
    text = text.group()
    return text[1:len(text)-1]

def hashtag(text):
    text = text.group()
    hashtag_body = text[1:]
    result = u"hashtag {} allcaps".format(hashtag_body)
    return result

def clean_hindi(text, for_embeddings=False,remove_stopwords=False, remove_punctuations=False):
    def re_sub(pattern, repl):
        return re.sub(pattern, repl, text)

    text= re.sub(r":\w+:",subs, text)
    text=  emoji.demojize(text)
    text = re_sub(r"#\S+", hashtag)
    text = re_sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*", "website")

    if for_embeddings:
        text = re_sub(r"@\w+", "user")
    else:
        text = text.replace('@', '')
    text = re_sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", "number")

    # Remove some special characters, or noise charater, but do not remove all!!
    if remove_punctuations:
        text = re.sub(r'([\.\'\"\/\-\_\--\_])',' ', text)
    else:
        clean_sent= re.sub(r'([\;\|•«\n])',' ', text)

    FLAG_remove_non_ascii =False
    if FLAG_remove_non_ascii:
        return clean_sent.encode("ascii", errors="ignore").decode().strip().lower()
    else:
        return clean_sent.strip()

total = []
for tweet in training_data["Post"]:
  total += clean_hindi(tweet).split()

len(total)

len(set(total))

token_length = [len(each) for each in list(set(total))]

plt.figure(figsize=(20,7))
sns.barplot(x = np.unique(np.sort(token_length),return_counts=True)[0],
            y= np.unique(np.sort(token_length),return_counts=True)[1])

tokens_length = pd.Series(token_length)
tokens_set = pd.Series(list(set(total)))

tokens_set.loc[tokens_length == 1]

tokens_set.loc[tokens_length == 2]

tokens_set.loc[tokens_length == 3]

tokens_set.loc[tokens_length == 10]

cnts_for_each_token = dict.fromkeys(set(total),0)

def tokens_(x):
  for each_token in clean_hindi(x).split():
    if len(each_token) >= 4 and len(each_token) <=10:
      cnts_for_each_token[each_token] +=1

training_data['Post'].apply(lambda x: tokens_(x))

cnts = pd.Series(cnts_for_each_token)
cnts[cnts > 1]

new_index = []
for each in cnts[cnts>1].index.to_list():
  new_index.append(re.sub(r'([\.\!\?\,\:\'\"\/\-\_\--\_])',"",each))

len(set(new_index))

np.sort(list(cnts_for_each_token.values()))[::-1]

np.argsort(list(cnts_for_each_token.values()))[::-1]

list(cnts_for_each_token.keys())[14081]

###sns.barplot(x = np.sort(list(cnts_for_each_token.values()))[::-1],
           ### y = list(cnts_for_each_token.keys())[np.argsort(list(cnts_for_each_token.values()))[::-1].to_list()])

"""# Augmentation

"""

"""aam_all_synsets = iwn.synsets('आम') # Mango
aam = aam_all_synsets[0]
aam.lemma_names()

iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.HINDI)
lemmas_ = dict.fromkeys(set(new_index),"")
for each in list(set(new_index))[0:10]:
  try:
    iwn.synsets(each)[0].lemma_names()
  except KeyError:
    pass

for each in list(set(new_index))[0:10]:
  try:
    random.iwn.synsets(each)[0].lemma_names().remove(each)
  except KeyError:
    pass

x= ['जीत', 'विजय', 'जय', 'सफलता', 'जयश्री', 'विजयश्री', 'फतह', 'अभिजय', 'अभिभावन']
[  print(each) for each in x if each not in ['जय']]

clean_hindi(training_data.iloc[3]["Post"])

training_data.iloc[0]["Post"]

clean_hindi("ia sanka 090@$^")

"""# BERT MODEL TRAINING"""

# specify GPU
device = torch.device("cuda")

import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = '[CLS] I want to [MASK] the car because it is cheap . [SEP]'
tokenized_text = tokenizer.tokenize(text)
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

# Create the segments tensors.
segments_ids = [0] * len(tokenized_text)

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
masked_index = tokenized_text.index('[MASK]')
# Load pre-trained model (weights)
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()

# Predict all tokens
with torch.no_grad():
    predictions = model(tokens_tensor, segments_tensors)

predicted_index = torch.argmax(predictions[0][0][masked_index]).item()
predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]

print(predicted_token)

from transformers import BertTokenizer, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()

sm = torch.nn.Softmax(dim=0)
torch.set_grad_enabled(False)

# set sentence with MASK token, convert to token_ids
sentence = '[CLS] In interviews, despots are often surprsingly [MASK] ; this helps to explain how seemingly awful people are able to command so many followers. [SEP]'
token_ids = tokenizer.encode(sentence, return_tensors='pt')
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()

# forward
output = model(token_ids)
last_hidden_state = output[0].squeeze(0)

mask_hidden_state = last_hidden_state[masked_position]

probs = sm(mask_hidden_state)

tokenizer.convert_ids_to_tokens([torch.argmax(probs).item()])[0]

bert = AutoModel.from_pretrained('bert-base-multilingual-cased')

# Load the BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')

seq_len = [len(i.split()) for i in training_data["Post"]]

pd.Series(seq_len).hist(bins = 30)

tokens_train = tokenizer.batch_encode_plus(
    training_data["Post"].apply(lambda x: clean_hindi(x)).tolist(),
    max_length = 64,
    padding=True,
    truncation=True
)

train_seq = torch.tensor(tokens_train['input_ids'])
train_mask = torch.tensor(tokens_train['attention_mask'])
train_y = torch.tensor(training_data["coarse_target"].tolist())

#define a batch size
batch_size = 32

# wrap tensors
train_data = TensorDataset(train_seq, train_mask, train_y)
train_sampler = RandomSampler(train_data)

# dataLoader for train set
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

valid_data = pd.read_csv("/content/Constraint_Hindi_Valid - Sheet1.csv")
valid_data.head()

valid_data["label"] = valid_data['Labels Set'].apply(lambda x: 0 if x == "non-hostile" else 1)

tokens_valid = tokenizer.batch_encode_plus(
    valid_data["Post"].apply(lambda x : clean_hindi(x)).tolist(),
    max_length = 64,
    padding=True,
    truncation=True
)

valid_seq = torch.tensor(tokens_valid['input_ids'])
valid_mask = torch.tensor(tokens_valid['attention_mask'])
valid_y = torch.tensor(valid_data["label"].tolist())

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

#define a batch size
batch_size = 32

# wrap tensors
valid_d = TensorDataset(valid_seq, valid_mask, valid_y)
valid_sampler = SequentialSampler(valid_d)

# dataLoader for train set
valid_dataloader = DataLoader(valid_d, sampler=valid_sampler, batch_size=batch_size)

for param in list(bert.parameters())[0:-50]:
    param.requires_grad = False

class BERT_Arch(nn.Module):

    def __init__(self, bert):

      super(BERT_Arch, self).__init__()
      self.bert = bert

      # dropout layer
      self.dropout = nn.Dropout(0.1)

      # relu activation function
      self.relu =  nn.ReLU()
      # dense layer 1
      self.fc1 = nn.Linear(768,512)

      # dense layer 2 (Output layer)
      self.fc2 = nn.Linear(512,2)
      #softmax activation function
      self.softmax = nn.LogSoftmax(dim=1)

    #define the forward pass
    def forward(self, sent_id, mask):

      #pass the inputs to the model
      cls_hs = self.bert(sent_id, attention_mask=mask)[1]

      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)

      # output layer
      x = self.fc2(x)

      # apply softmax activation
      x = self.softmax(x)

      return x

model1 = BERT_Arch(bert)
# push the model to GPU
model1 = model1.to(device)

from transformers import AdamW

# define the optimizer
optimizer = AdamW(model1.parameters(),
                  lr = 1e-5)

cross_entropy  = nn.NLLLoss()

n_epochs_stop = 6
epochs_no_improve = 0
early_stop = False

train_losses = []
valid_losses = []
avg_train_losses = []
avg_valid_losses = []

min_val_loss = np.Inf
# number of training epochs
epochs = 10

for epoch in range(1, epochs+1):
  print('\n Epoch {:} / {:}'.format(epoch , epochs))
  model1.train()
  for step,batch in enumerate(train_dataloader):
    if step % 50 == 0 and not step == 0:
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))
    batch = [r.to(device) for r in batch]
    sent_id, mask, labels = batch
    model1.zero_grad()
    preds = model1(sent_id, mask)
    loss = cross_entropy(preds, labels)
    train_losses.append(loss.item())
    loss.backward()

    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
    torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)
    optimizer.step()
    #preds=preds.detach().cpu().numpy()

  model1.eval() # prep model for evaluation
  for step,batch in enumerate(valid_dataloader):
    with torch.no_grad():
      batch = [r.to(device) for r in batch]
      sent_id, mask, labels = batch
      output = model1(sent_id, mask)
      loss = cross_entropy(output, labels)
      valid_losses.append(loss.item())

  train_loss = np.average(train_losses)
  valid_loss = np.average(valid_losses)
  avg_train_losses.append(train_loss)
  avg_valid_losses.append(valid_loss)

  epoch_len = len(str(epochs))
  print(f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +
                     f'train_loss: {train_loss:.5f} ' +
                     f'valid_loss: {valid_loss:.5f}')

  train_losses = []
  valid_losses = []

  if valid_loss < min_val_loss:
    epochs_no_improve = 0
    min_val_loss = valid_loss
    torch.save(model1.state_dict(), '/content/drive/MyDrive/saved_weights.pt')
  else:
    epochs_no_improve += 1

  if epoch > 5 and epochs_no_improve == n_epochs_stop:
    print('Early stopping!' )
    early_stop = True
    break
  else:
    continue

# visualize the loss as the network trained
plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')
plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')

# find position of lowest validation loss
minposs = avg_valid_losses.index(min(avg_valid_losses))+1
plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.ylim(0, 0.5) # consistent scale
plt.xlim(0, len(avg_train_losses)+1) # consistent scale
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""# XLM ROBERTA TRAINING"""

xlmr = AutoModel.from_pretrained("xlm-roberta-base")

# Load the XLM-ROBERTA tokenizer
xlmr_tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')

xlmr_tokens_train = xlmr_tokenizer.batch_encode_plus(
    training_data["Post"].apply(lambda x : clean_hindi(x)).tolist(),
    max_length = 64,
    padding=True,
    truncation=True
)

xlmr_train_seq = torch.tensor(xlmr_tokens_train['input_ids'])
xlmr_train_mask = torch.tensor(xlmr_tokens_train['attention_mask'])
xlmr_train_y = torch.tensor(training_data["coarse_target"].tolist())

#define a batch size
batch_size = 32

# wrap tensors
xlmr_train_data = TensorDataset(xlmr_train_seq, xlmr_train_mask, xlmr_train_y)
xlmr_train_sampler = RandomSampler(xlmr_train_data)

# dataLoader for train set
xlmr_train_dataloader = DataLoader(xlmr_train_data, sampler=xlmr_train_sampler, batch_size=batch_size)

xlmr_tokens_valid = xlmr_tokenizer.batch_encode_plus(
    valid_data["Post"].apply(lambda x: clean_hindi(x)).tolist(),
    max_length = 64,
    padding=True,
    truncation=True
)

xlmr_valid_seq = torch.tensor(xlmr_tokens_valid['input_ids'])
xlmr_valid_mask = torch.tensor(xlmr_tokens_valid['attention_mask'])
xlmr_valid_y = torch.tensor(valid_data["label"].tolist())

#from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

#define a batch size
batch_size = 32

# wrap tensors
xlmr_valid_d = TensorDataset(xlmr_valid_seq, xlmr_valid_mask, xlmr_valid_y)
xlmr_valid_sampler = SequentialSampler(xlmr_valid_d)

# dataLoader for train set
xlmr_valid_dataloader = DataLoader(xlmr_valid_d, sampler=xlmr_valid_sampler, batch_size=batch_size)

for param in list(xlmr.parameters())[0:-50]:
    param.requires_grad = False

class XLMR_Arch(nn.Module):

    def __init__(self, xlmr):

      super(XLMR_Arch, self).__init__()
      self.xlmr = xlmr

      # dropout layer
      self.dropout = nn.Dropout(0.1)

      # relu activation function
      self.relu =  nn.ReLU()
      # dense layer 1
      self.fc1 = nn.Linear(768,512)

      # dense layer 2 (Output layer)
      self.fc2 = nn.Linear(512,2)
      #softmax activation function
      self.softmax = nn.LogSoftmax(dim=1)

    #define the forward pass
    def forward(self, sent_id, mask):

      #pass the inputs to the model
      cls_hs = self.xlmr(sent_id, attention_mask=mask)[1]

      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)

      # output layer
      x = self.fc2(x)

      # apply softmax activation
      x = self.softmax(x)

      return x

model2 = XLMR_Arch(xlmr)
# push the model to GPU
model2 = model2.to(device)

from transformers import AdamW

# define the optimizer
optimizer = AdamW(model2.parameters(),
                  lr = 1e-5)

cross_entropy  = nn.NLLLoss()

n_epochs_stop = 6
epochs_no_improve = 0
early_stop = False

train_losses = []
valid_losses = []
xlmr_avg_train_losses = []
xlmr_avg_valid_losses = []

min_val_loss = np.Inf

# number of training epochs
epochs = 10

for epoch in range(1, epochs+1):
  print('\n Epoch {:} / {:}'.format(epoch , epochs))
  model2.train()
  for step,batch in enumerate(xlmr_train_dataloader):
    if step % 50 == 0 and not step == 0:
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(xlmr_train_dataloader)))
    batch = [r.to(device) for r in batch]
    sent_id, mask, labels = batch
    model2.zero_grad()
    preds = model2(sent_id, mask)
    loss = cross_entropy(preds, labels)
    train_losses.append(loss.item())
    loss.backward()

    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
    torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)
    optimizer.step()
    #preds=preds.detach().cpu().numpy()

  model2.eval() # prep model for evaluation
  for step,batch in enumerate(xlmr_valid_dataloader):
    with torch.no_grad():
      batch = [r.to(device) for r in batch]
      sent_id, mask, labels = batch
      output = model2(sent_id, mask)
      loss = cross_entropy(output, labels)
      valid_losses.append(loss.item())

  train_loss = np.average(train_losses)
  valid_loss = np.average(valid_losses)
  xlmr_avg_train_losses.append(train_loss)
  xlmr_avg_valid_losses.append(valid_loss)

  epoch_len = len(str(epochs))
  print(f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +
                     f'train_loss: {train_loss:.5f} ' +
                     f'valid_loss: {valid_loss:.5f}')

  train_losses = []
  valid_losses = []

  if valid_loss < min_val_loss:
    epochs_no_improve = 0
    min_val_loss = valid_loss
    torch.save(model2.state_dict(), '/content/drive/MyDrive/xlmr_saved_weights.pt')
  else:
    epochs_no_improve += 1

  if epoch > 5 and epochs_no_improve == n_epochs_stop:
    print('Early stopping!' )
    early_stop = True
    break
  else:
    continue

"""# Hybrid Model"""

model1.load_state_dict(torch.load("/content/drive/MyDrive/saved_weights.pt"))
model2.load_state_dict(torch.load("/content/drive/MyDrive/xlmr_saved_weights.pt"))

def transform_hybrid(model,df,col):
  if model == "bert":
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
  else :
    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')

  tokens_valid = tokenizer.batch_encode_plus(
      df["Post"].apply(lambda x: clean_hindi(x)).tolist(),
      max_length = 64,
      padding=True,
      truncation=True
  )

  seq = torch.tensor(tokens_valid['input_ids'])
  mask = torch.tensor(tokens_valid['attention_mask'])
  y = torch.tensor(df[col].tolist())

  d = TensorDataset(seq, mask, y)
  sampler = SequentialSampler(d)
  return d,sampler

batch_size = 32
hyb_train_bert_d,hyb_train_bert_sample = transform_hybrid("bert",training_data,"coarse_target")
# dataLoader for train set
hyb_train_bert_dataloader = DataLoader(hyb_train_bert_d,
                                       sampler=hyb_train_bert_sample, batch_size=batch_size)

hyb_valid_bert_d,hyb_valid_bert_sample = transform_hybrid("bert",valid_data,"label")
# dataLoader for train set
hyb_valid_bert_dataloader = DataLoader(hyb_valid_bert_d,
                                       sampler=hyb_valid_bert_sample, batch_size=batch_size)

batch_size = 32
hyb_train_xlmr_d,hyb_train_xlmr_sample = transform_hybrid("xlmr",training_data,"coarse_target")
# dataLoader for train set
hyb_train_xlmr_dataloader = DataLoader(hyb_train_xlmr_d,
                                       sampler=hyb_train_xlmr_sample, batch_size=batch_size)

hyb_valid_xlmr_d,hyb_valid_xlmr_sample = transform_hybrid("xlmr",valid_data,"label")
# dataLoader for train set
hyb_valid_xlmr_dataloader = DataLoader(hyb_valid_xlmr_d,
                                       sampler=hyb_valid_xlmr_sample, batch_size=batch_size)

def hook1(module, input, output):
    outputs1.append(output)
model1.fc1.register_forward_hook(hook1)

def hook2(module, input, output):
    outputs2.append(output)
model2.fc1.register_forward_hook(hook2)

outputs1= []
model1.eval() # prep model for evaluation
def embeddings_(dataloader):
  for step,batch in enumerate(dataloader):
    with torch.no_grad():
      batch = [r.to(device) for r in batch]
      sent_id, mask, labels = batch
      out = model1(sent_id, mask)

embeddings_(hyb_train_bert_dataloader)
embeddings_(hyb_valid_bert_dataloader)

outputs2= []
model2.eval() # prep model for evaluation
def embeddings_(dataloader):
  for step,batch in enumerate(dataloader):
    with torch.no_grad():
      batch = [r.to(device) for r in batch]
      sent_id, mask, labels = batch
      out = model2(sent_id, mask)

embeddings_(hyb_train_xlmr_dataloader)
embeddings_(hyb_valid_xlmr_dataloader)

class Hybrid(nn.Module):
  def __init__(self):
    super(Hybrid, self).__init__()
    self.dropout = nn.Dropout(0.1)
    # relu activation function
    self.relu =  nn.ReLU()
    # dense layer 1
    self.fc1 = nn.Linear(1024,768)
    self.fc2 = nn.Linear(768,512)
    # dense layer 3 (Output layer)
    self.fc3 = nn.Linear(512,2)
    #softmax activation function
    self.softmax = nn.LogSoftmax(dim=1)

  #define the forward pass
  def forward(self,x):
    x = self.fc1(x)
    x = self.fc2(x)
    x = self.relu(x)
    x = self.dropout(x)
    # output layer
    x = self.fc3(x)
    # apply softmax activation
    x = self.softmax(x)
    return x

model3 = Hybrid()
# push the model to GPU
model3 = model3.to(device)

from transformers import AdamW

# define the optimizer
optimizer = AdamW(model3.parameters(),
                  lr = 1e-5)

cross_entropy  = nn.NLLLoss()

n_epochs_stop = 6
epochs_no_improve = 0
early_stop = False

train_losses = []
valid_losses = []
xlmr_avg_train_losses = []
xlmr_avg_valid_losses = []

min_val_loss = np.Inf

# number of training epochs
epochs = 10

for epoch in range(1, epochs+1):
  print('\n Epoch {:} / {:}'.format(epoch , epochs))
  model3.train()
  for step,batch in enumerate(hyb_train_xlmr_dataloader):
    if step % 50 == 0 and not step == 0:
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(hyb_train_xlmr_dataloader)))
    batch = [r.to(device) for r in batch]
    sent_id, mask, labels = batch
    model3.zero_grad()
    preds = model3(torch.cat((outputs1[step], outputs2[step]), 1))
    loss = cross_entropy(preds, labels)
    train_losses.append(loss.item())
    loss.backward()

    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
    torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.0)
    optimizer.step()
    #preds=preds.detach().cpu().numpy()

  model3.eval() # prep model for evaluation
  for step,batch in enumerate(hyb_valid_xlmr_dataloader):
    with torch.no_grad():
      batch = [r.to(device) for r in batch]
      sent_id, mask, labels = batch
      output = model3(torch.cat((outputs1[len(hyb_train_xlmr_dataloader)+step],
                                 outputs2[len(hyb_train_xlmr_dataloader)+step]), 1))
      loss = cross_entropy(output, labels)
      valid_losses.append(loss.item())

  train_loss = np.average(train_losses)
  valid_loss = np.average(valid_losses)
  xlmr_avg_train_losses.append(train_loss)
  xlmr_avg_valid_losses.append(valid_loss)

  epoch_len = len(str(epochs))
  print(f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +
                     f'train_loss: {train_loss:.5f} ' +
                     f'valid_loss: {valid_loss:.5f}')

  train_losses = []
  valid_losses = []

  if valid_loss < min_val_loss:
    epochs_no_improve = 0
    min_val_loss = valid_loss
    torch.save(model3.state_dict(), '/content/drive/MyDrive/hybrid_saved_weights.pt')
  else:
    epochs_no_improve += 1

  if epoch > 5 and epochs_no_improve == n_epochs_stop:
    print('Early stopping!' )
    early_stop = True
    break
  else:
    continue

"""# Coarse Classification"""

test_data = pd.read_csv("/content/Test Set Complete - test.csv")
test_data.head()

test_data["label"] = test_data['Labels Set'].apply(lambda x: 0 if x == "non-hostile" else 1)

def transform_(model):
  if model == "bert":
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
  else :
    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')

  tokens_valid = tokenizer.batch_encode_plus(
      test_data["Post"].apply(lambda x: clean_hindi(x)).tolist(),
      max_length = 64,
      padding=True,
      truncation=True
  )

  seq = torch.tensor(tokens_valid['input_ids'])
  mask = torch.tensor(tokens_valid['attention_mask'])
  y = torch.tensor(test_data["label"].tolist())

  return seq, mask,y

#define a batch size
batch_size = 32

# wrap tensors
bert_test_seq,bert_test_mask , bert_test_y = transform_("bert")

#define a batch size
batch_size = 32

# wrap tensors
xlmr_test_seq,xlmr_test_mask , xlmr_test_y = transform_("xlmr")

path = '/content/drive/MyDrive/saved_weights.pt'
model1.load_state_dict(torch.load(path))
with torch.no_grad():
  preds = model1(bert_test_seq.to(device), bert_test_mask.to(device))
  preds = preds.detach().cpu().numpy()
preds = np.argmax(preds, axis = 1)
print(classification_report(bert_test_y, preds))

path = '/content/drive/MyDrive/xlmr_saved_weights.pt'
model2.load_state_dict(torch.load(path))
with torch.no_grad():
  preds = model2(xlmr_test_seq.to(device), xlmr_test_mask.to(device))
  preds = preds.detach().cpu().numpy()
preds = np.argmax(preds, axis = 1)
print(classification_report(xlmr_test_y, preds))

outputs1= []
outputs2= []
model1.eval() # prep model for evaluation
model2.eval()
def test_embeddings_(model):
  with torch.no_grad():
    if model == "bert":
      out = model1(bert_test_seq.to(device), bert_test_mask.to(device))
    else :
      out = model2(xlmr_test_seq.to(device), xlmr_test_mask.to(device))
test_embeddings_("bert")
test_embeddings_("xlmr")

path = '/content/drive/MyDrive/hybrid_saved_weights.pt'
model3.load_state_dict(torch.load(path))
with torch.no_grad():
  preds = model3(torch.cat((outputs1[0], outputs2[0]), 1))
  preds = preds.detach().cpu().numpy()
preds = np.argmax(preds, axis = 1)
print(classification_report(xlmr_test_y, preds))

"""# Fine grained Classification

"""

'''for param in list(model1.parameters())[0:-1]:
  param.requires_grad = True

tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
tokens_train = tokenizer.batch_encode_plus(
    training_data["Post"].apply(lambda x: clean_hindi(x)).tolist(),
    max_length = 64,
    padding=True,
    truncation=True
)

train_seq = torch.tensor(tokens_train['input_ids'])
train_mask = torch.tensor(tokens_train['attention_mask'])
train_y = torch.tensor(training_data[['defamation', 'fake', 'hate', 'offensive']].values)

#define a batch size
batch_size = 32

# wrap tensors
train_data = TensorDataset(train_seq, train_mask, train_y)
train_sampler = RandomSampler(train_data)

# dataLoader for train set
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

for each in ['defamation', 'fake', 'hate', 'offensive']:
  valid_data[each]= valid_data['Labels Set'].str.contains(each).apply(lambda x : 1 if x== True else 0)

tokens_valid = tokenizer.batch_encode_plus(
    valid_data["Post"].apply(lambda x : clean_hindi(x)).tolist(),
    max_length = 64,
    padding=True,
    truncation=True
)

valid_seq = torch.tensor(tokens_valid['input_ids'])
valid_mask = torch.tensor(tokens_valid['attention_mask'])
valid_y = torch.tensor(valid_data[['defamation', 'fake', 'hate', 'offensive']].values)

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

#define a batch size
batch_size = 32

# wrap tensors
valid_d = TensorDataset(valid_seq, valid_mask, valid_y)
valid_sampler = SequentialSampler(valid_d)

# dataLoader for train set
valid_dataloader = DataLoader(valid_d, sampler=valid_sampler, batch_size=batch_size)

'''class Fine(nn.Module):
  def __init__(self):
    super(Fine, self).__init__()
    modules = list(model1.children())[0:-1]
    self.fine_grained = torch.nn.Sequential(*modules)
    self.linear = nn.Linear(in_features=512  ,out_features=4)
    self.softmax = nn.LogSoftmax(dim=4)
  def forward(self, sent_id, mask):
    cls_hs = self.fine_grained(sent_id, attention_mask=mask)[1]
    x = self.linear(cls_hs)
    x = self.softmax(x)
    return x

class Fine(nn.Module):

    def __init__(self, bert):

      super(Fine, self).__init__()
      self.bert = bert

      # dropout layer
      self.dropout = nn.Dropout(0.1)

      # relu activation function
      self.relu =  nn.ReLU()
      # dense layer 1
      self.fc1 = nn.Linear(768,512)

      # dense layer 2 (Output layer)
      self.fc2 = nn.Linear(512,4)
      #softmax activation function
      self.sigmoid = nn.Sigmoid()

    #define the forward pass
    def forward(self, sent_id, mask):

      #pass the inputs to the model
      cls_hs = self.bert(sent_id, attention_mask=mask)[1]

      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)

      # output layer
      x = self.fc2(x)

      # apply softmax activation
      x = self.sigmoid(x)

      return x

'''model1 = BERT_Arch(bert)
# push the model to GPU
model1 = model1.to(device)

model4 = Fine(bert)
# push the model to GPU
model4 = model4.to(device)

from transformers import AdamW

# define the optimizer
optimizer = AdamW(model1.parameters(),
                  lr = 1e-5)

criterion  = nn.BCELoss()

n_epochs_stop = 6
epochs_no_improve = 0
early_stop = False

train_losses = []
valid_losses = []
avg_train_losses = []
avg_valid_losses = []

min_val_loss = np.Inf
# number of training epochs
epochs = 10

for epoch in range(1, epochs+1):
  print('\n Epoch {:} / {:}'.format(epoch , epochs))
  model4.train()
  for step,batch in enumerate(train_dataloader):
    if step % 50 == 0 and not step == 0:
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))
    batch = [r.to(device) for r in batch]
    sent_id, mask, labels = batch
    model4.zero_grad()
    #print(sent_id.size(),mask.size())
    preds = model4(sent_id, mask)
    #sigmoid = nn.Sigmoid()
    loss = criterion(preds, labels.type(torch.cuda.FloatTensor))
    train_losses.append(loss.item())
    loss.backward()

    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
    torch.nn.utils.clip_grad_norm_(model4.parameters(), 1.0)
    optimizer.step()
    #preds=preds.detach().cpu().numpy()

  model4.eval() # prep model for evaluation
  for step,batch in enumerate(valid_dataloader):
    with torch.no_grad():
      batch = [r.to(device) for r in batch]
      sent_id, mask, labels = batch
      output = model4(sent_id, mask)
      loss = criterion(output, labels.type(torch.cuda.FloatTensor))
      valid_losses.append(loss.item())

  train_loss = np.average(train_losses)
  valid_loss = np.average(valid_losses)
  avg_train_losses.append(train_loss)
  avg_valid_losses.append(valid_loss)

  epoch_len = len(str(epochs))
  print(f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +
                     f'train_loss: {train_loss:.5f} ' +
                     f'valid_loss: {valid_loss:.5f}')

  train_losses = []
  valid_losses = []

  if valid_loss < min_val_loss:
    epochs_no_improve = 0
    min_val_loss = valid_loss
    torch.save(model4.state_dict(), 'fine_grained_saved_weights.pt')
  else:
    epochs_no_improve += 1

  if epoch > 5 and epochs_no_improve == n_epochs_stop:
    print('Early stopping!' )
    early_stop = True
    break
  else:
    continue

for each in ['defamation', 'fake', 'hate', 'offensive']:
  test_data[each]= test_data['Labels Set'].str.contains(each).apply(lambda x : 1 if x== True else 0)

tokens_test = tokenizer.batch_encode_plus(
      test_data["Post"].apply(lambda x: clean_hindi(x)).tolist(),
      max_length = 64,
      padding=True,
      truncation=True)

seq = torch.tensor(tokens_test['input_ids'])
mask = torch.tensor(tokens_test['attention_mask'])
y = torch.tensor(test_data[['defamation', 'fake', 'hate', 'offensive']].values)

path = 'fine_grained_saved_weights.pt'
model4.load_state_dict(torch.load(path))
with torch.no_grad():
  preds = model4(seq.to(device), mask.to(device))
  preds = preds.detach().cpu().numpy()

preds

y

preds.round()

print(classification_report(y,preds.round()))

from sklearn.metrics import  accuracy_score
accuracy_score(y,preds.round())

